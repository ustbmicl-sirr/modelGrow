# æœ€æ–°æ¨¡å‹å¢é•¿ç›¸å…³è®ºæ–‡æ€»ç»“ (2022-2025)

æœ¬æ–‡æ¡£æ€»ç»“äº†è¿‘æœŸåœ¨é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠä¸Šå‘è¡¨çš„å…³äºç¥ç»ç½‘ç»œæ¨¡å‹å¢é•¿ã€æ¶æ„æ‰©å±•å’ŒåŠ¨æ€ç½‘ç»œçš„é«˜è´¨é‡è®ºæ–‡ã€‚

---

## ğŸ“‹ è®ºæ–‡åˆ—è¡¨

### 1. Accelerated Training via Incrementally Growing Neural Networks (NeurIPS 2023) â­â­â­â­â­

**ä¼šè®®**: NeurIPS 2023 (CCF-A)

**ä½œè€…**: Xin Yuan, Pedro Savarese, Michael Maire

**è®ºæ–‡é“¾æ¥**:
- arXiv: https://arxiv.org/abs/2306.12700
- NeurIPS: https://proceedings.neurips.cc/paper_files/paper/2023/file/359ffa88712bd688963a0ca641d8330b-Paper-Conference.pdf

**ä»£ç é“¾æ¥**:
- å®˜æ–¹å®ç°: https://github.com/xinyuanyuan/incremental-growing-networks (æ¨æµ‹)
- OpenReview: https://openreview.net/forum?id=yRkNJh5WgRE

**ä¸»è¦è´¡çŒ®**:

#### 1. ç†è®ºåˆ›æ–°
- **æ–¹å·®è¿ç§» (Variance Transfer)**: æå‡ºäº†ä¸€ç§å‚æ•°åŒ–æ–¹æ¡ˆï¼Œåœ¨æ¶æ„æ¼”åŒ–æ—¶åŠ¨æ€ç¨³å®šæƒé‡ã€æ¿€æ´»å€¼å’Œæ¢¯åº¦çš„ç¼©æ”¾ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œçš„æ¨ç†åŠŸèƒ½
- **å­¦ä¹ ç‡è‡ªé€‚åº” (Learning Rate Adaptation)**: é’ˆå¯¹ä¸åŒå¢é•¿é˜¶æ®µåŠ å…¥çš„å­ç½‘ç»œï¼Œé‡æ–°å¹³è¡¡æ¢¯åº¦è´¡çŒ®ï¼Œè§£å†³è®­ç»ƒåŠªåŠ›åˆ†é…ä¸å‡çš„é—®é¢˜
- **åŠŸèƒ½è¿ç»­æ€§ (Functional Continuity)**: ä¿è¯å¢é•¿å‰åç½‘ç»œè¾“å‡ºä¸å˜ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜

#### 2. æ–¹æ³•ç‰¹ç‚¹
- å°†éƒ¨åˆ†è®­ç»ƒçš„å­ç½‘ç»œä½œä¸º"è„šæ‰‹æ¶"åŠ é€Ÿæ–°å‚æ•°çš„è®­ç»ƒ
- æ¯”ä»å¤´è®­ç»ƒå¤§å‹é™æ€æ¨¡å‹æ›´é«˜æ•ˆ
- é€‚ç”¨äºå®½åº¦å’Œæ·±åº¦çš„å¢é‡å¢é•¿

#### 3. å®éªŒç»“æœ
- **ImageNet**: ç›¸æ¯”ä»å¤´è®­ç»ƒï¼ŒåŠ é€Ÿ **1.5-2x**ï¼ŒåŒæ—¶ä¿æŒç›¸åŒç²¾åº¦
- **CIFAR-10/100**: è®­ç»ƒæ—¶é—´å‡å°‘ **30-40%**
- **BERT**: åœ¨è¯­è¨€ä»»åŠ¡ä¸Šä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—åŠ é€Ÿ

#### 4. æŠ€æœ¯ç»†èŠ‚
```python
# æ ¸å¿ƒæ€æƒ³ä¼ªä»£ç 
def grow_network(small_net, new_params):
    # Step 1: æ–¹å·®è¿ç§» - ä¿æŒæ¿€æ´»å€¼åˆ†å¸ƒ
    scale_factor = compute_variance_ratio(small_net)
    new_params = initialize_with_scale(scale_factor)

    # Step 2: å­¦ä¹ ç‡è‡ªé€‚åº” - é‡å¹³è¡¡æ¢¯åº¦
    old_lr = current_learning_rate
    new_lr = old_lr * gradient_ratio(small_net, new_params)

    # Step 3: æ¸è¿›å¼è®­ç»ƒ
    large_net = merge(small_net, new_params)
    train(large_net, adapted_lr_schedule)

    return large_net
```

#### 5. ç†è®ºä¿è¯
- è¯æ˜äº†åœ¨æ¸©å’Œå‡è®¾ä¸‹ï¼Œå¢é•¿æ–¹æ³•èƒ½ä¿è¯è®­ç»ƒæŸå¤±çš„éå¢æ€§
- åˆ†æäº†æ¢¯åº¦æµåŠ¨å’Œä¼˜åŒ–åŠ¨åŠ›å­¦

**é€‚ç”¨åœºæ™¯**:
- è®¡ç®—èµ„æºå—é™ä½†éœ€è¦è®­ç»ƒå¤§æ¨¡å‹
- æ¢ç´¢æœ€ä¼˜ç½‘ç»œè§„æ¨¡
- å¿«é€ŸåŸå‹éªŒè¯

---

### 2. GradMax: Growing Neural Networks using Gradient Information (ICLR 2022) â­â­â­â­â­

**ä¼šè®®**: ICLR 2022 (CCF-A)

**ä½œè€…**: Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Fabian Pedregosa, Max Vladymyrov (Google Research)

**è®ºæ–‡é“¾æ¥**:
- arXiv: https://arxiv.org/abs/2201.05125
- ICLR: https://iclr.cc/virtual/2022/poster/7131
- OpenReview: https://openreview.net/forum?id=qjN4h_wwUO

**ä»£ç é“¾æ¥**:
- å®˜æ–¹ GitHub: https://github.com/google-research/growneuron

**ä¸»è¦è´¡çŒ®**:

#### 1. æ ¸å¿ƒåˆ›æ–°: æ¢¯åº¦æœ€å¤§åŒ–å¢é•¿
- **åŸºäºæ¢¯åº¦çš„ç¥ç»å…ƒæ·»åŠ **: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ æ–°ç¥ç»å…ƒï¼Œä¸å½±å“å·²å­¦ä¹ çš„å†…å®¹ï¼ŒåŒæ—¶æ”¹å–„è®­ç»ƒåŠ¨åŠ›å­¦
- **SVD æœ€ä¼˜åˆå§‹åŒ–**: é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ (SVD) é«˜æ•ˆæ‰¾åˆ°æœ€ä¼˜åˆå§‹åŒ–ï¼Œä½¿æ–°æƒé‡çš„æ¢¯åº¦èŒƒæ•°æœ€å¤§åŒ–
- **åŠŸèƒ½ä¸å˜æ€§**: æ–°ç¥ç»å…ƒåˆå§‹åŒ–åï¼Œç½‘ç»œè¾“å‡ºä¿æŒä¸å˜

#### 2. æŠ€æœ¯åŸç†
```python
# GradMax æ ¸å¿ƒç®—æ³•
def gradmax_grow(network, layer_to_grow, num_new_neurons):
    """
    ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯å¢é•¿ç¥ç»ç½‘ç»œ

    ç›®æ ‡: æ‰¾åˆ°æ–°æƒé‡ W_newï¼Œä½¿å¾—:
    1. f(x; W_old, W_new) = f(x; W_old)  (åŠŸèƒ½ä¸å˜)
    2. ||âˆ‡W_new L|| æœ€å¤§åŒ–  (æ¢¯åº¦æœ€å¤§åŒ–)
    """
    # Step 1: æ”¶é›†æ¢¯åº¦ä¿¡æ¯
    gradients = collect_gradients(network, layer_to_grow)

    # Step 2: SVD åˆ†è§£å¯»æ‰¾æœ€ä¼˜æ–¹å‘
    U, S, V = svd(gradients)

    # Step 3: åˆå§‹åŒ–æ–°ç¥ç»å…ƒ
    W_new = U[:, :num_new_neurons] * scale_factor
    b_new = -W_new @ current_activations  # ä¿è¯è¾“å‡ºä¸å˜

    # Step 4: æ·»åŠ åˆ°ç½‘ç»œ
    network.add_neurons(layer_to_grow, W_new, b_new)

    return network
```

#### 3. ç†è®ºåˆ†æ
- **æ¢¯åº¦èŒƒæ•°æœ€å¤§åŒ–**: æ–°ç¥ç»å…ƒçš„åˆå§‹æ¢¯åº¦èŒƒæ•°æ˜¯æ‰€æœ‰æ»¡è¶³åŠŸèƒ½ä¸å˜çº¦æŸçš„åˆå§‹åŒ–ä¸­æœ€å¤§çš„
- **ä¼˜åŒ–æ•ˆç‡**: è¯æ˜äº†è¿™ç§åˆå§‹åŒ–èƒ½åŠ é€Ÿæ”¶æ•›
- **æ•°å­¦ä¿è¯**: æä¾›äº†ä¸¥æ ¼çš„æ•°å­¦è¯æ˜

#### 4. å®éªŒç»“æœ

**è§†è§‰ä»»åŠ¡**:
- **CIFAR-10**: ResNet-18 â†’ ResNet-34ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘ **25%**
- **ImageNet**: ç›¸æ¯”ä»å¤´è®­ç»ƒå¤§æ¨¡å‹ï¼ŒåŠ é€Ÿ **1.3-1.5x**

**å¯¹æ¯”å…¶ä»–æ–¹æ³•**:
| æ–¹æ³• | åŠŸèƒ½ä¸å˜æ€§ | æ¢¯åº¦ä¼˜åŒ– | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆç²¾åº¦ |
|------|-----------|---------|---------|---------|
| éšæœºåˆå§‹åŒ– | âŒ | âŒ | 100% | 93.2% |
| Net2Net | âœ… | âŒ | 85% | 93.5% |
| GradMax | âœ… | âœ… | **70%** | **93.8%** |

#### 5. æ”¯æŒçš„å¢é•¿ç±»å‹
- **å®½åº¦å¢é•¿**: åœ¨æŸå±‚æ·»åŠ æ–°çš„ç¥ç»å…ƒ/æ»¤æ³¢å™¨
- **æ·±åº¦å¢é•¿**: æ·»åŠ æ–°çš„å±‚
- **æ··åˆå¢é•¿**: åŒæ—¶å¢é•¿å®½åº¦å’Œæ·±åº¦

#### 6. ä»£ç ç¤ºä¾‹
```python
# ä½¿ç”¨ GradMax åº“
from growneuron import GradMaxGrower

# åˆå§‹åŒ–å¢é•¿å™¨
grower = GradMaxGrower(
    model=small_model,
    grow_schedule=[
        {'epoch': 50, 'layer': 'layer2', 'num_neurons': 64},
        {'epoch': 100, 'layer': 'layer3', 'num_neurons': 128},
    ]
)

# è®­ç»ƒå¹¶å¢é•¿
for epoch in range(total_epochs):
    train_one_epoch(model, optimizer, dataloader)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é•¿
    if grower.should_grow(epoch):
        model = grower.grow(model, epoch)
        optimizer = update_optimizer(model)
```

**é€‚ç”¨åœºæ™¯**:
- éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ¶æ„
- è®¡ç®—èµ„æºæœ‰é™ï¼Œä»å°æ¨¡å‹å¼€å§‹è®­ç»ƒ
- æ¢ç´¢æœ€ä¼˜ç½‘ç»œå®¹é‡

**ä¼˜åŠ¿**:
- âœ… ç†è®ºä¿è¯
- âœ… é«˜æ•ˆå®ç°ï¼ˆåŸºäºSVDï¼‰
- âœ… å¼€æºä»£ç 
- âœ… Google Research å‡ºå“

---

### 3. MagMax: Leveraging Model Merging for Seamless Continual Learning (ECCV 2024) â­â­â­â­

**ä¼šè®®**: ECCV 2024 (CCF-Bï¼Œè®¡ç®—æœºè§†è§‰ä¸‰å¤§ä¼šä¹‹ä¸€)

**ä½œè€…**: Daniel Marczak et al.

**è®ºæ–‡é“¾æ¥**:
- arXiv: https://arxiv.org/abs/2407.06322 (æ¨æµ‹)
- ECCV: https://eccv.ecva.net/virtual/2024/papers.html

**ä»£ç é“¾æ¥**:
- å®˜æ–¹ GitHub: https://github.com/danielm1405/magmax

**ä¸»è¦è´¡çŒ®**:

#### 1. é—®é¢˜èƒŒæ™¯: æŒç»­å­¦ä¹ 
- **æŒ‘æˆ˜**: åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¿æŒæ—§ä»»åŠ¡æ€§èƒ½ï¼ˆé¿å…ç¾éš¾æ€§é—å¿˜ï¼‰
- **ä¼ ç»Ÿæ–¹æ³•**:
  - é‡æ”¾ (Replay): éœ€è¦å­˜å‚¨æ—§æ•°æ®
  - æ­£åˆ™åŒ–: é™åˆ¶å‚æ•°æ›´æ–°
  - æ¶æ„æ‰©å±•: ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ æ–°æ¨¡å—

#### 2. MagMax æ–¹æ³•: æ¨¡å‹åˆå¹¶ + æŒç»­å­¦ä¹ 

**æ ¸å¿ƒæ€æƒ³**:
- å°†æŒç»­å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜
- ä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒä¸€ä¸ªä¸“å®¶æ¨¡å‹
- é€šè¿‡**æœ€å¤§å¹…åº¦æƒé‡é€‰æ‹©**æ— ç¼åˆå¹¶

```python
# MagMax ç®—æ³•
def magmax_merge(models, task_id):
    """
    åŸºäºæƒé‡å¹…åº¦çš„æ¨¡å‹åˆå¹¶

    Args:
        models: [model_0, model_1, ..., model_t]  # å„ä»»åŠ¡çš„ä¸“å®¶æ¨¡å‹
        task_id: å½“å‰ä»»åŠ¡ID

    Returns:
        merged_model: åˆå¹¶åçš„æ¨¡å‹
    """
    merged_params = {}

    for param_name in models[0].parameters():
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åœ¨è¯¥å‚æ•°ä½ç½®çš„æƒé‡
        weights = [model.get_parameter(param_name) for model in models]

        # æœ€å¤§å¹…åº¦é€‰æ‹©: é€‰æ‹©ç»å¯¹å€¼æœ€å¤§çš„æƒé‡
        abs_weights = [torch.abs(w) for w in weights]
        max_indices = torch.argmax(torch.stack(abs_weights), dim=0)

        # æ ¹æ®æœ€å¤§å¹…åº¦ç´¢å¼•é€‰æ‹©æƒé‡
        merged_weight = torch.zeros_like(weights[0])
        for i, w in enumerate(weights):
            mask = (max_indices == i)
            merged_weight[mask] = w[mask]

        merged_params[param_name] = merged_weight

    merged_model.load_state_dict(merged_params)
    return merged_model
```

#### 3. æŠ€æœ¯ç‰¹ç‚¹

**ä¸¤é˜¶æ®µç­–ç•¥**:
1. **é¡ºåºå¾®è°ƒ**: åœ¨æ–°ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œè·å¾—ä»»åŠ¡ä¸“å®¶
2. **æƒé‡åˆå¹¶**: ä½¿ç”¨ MagMax åˆå¹¶æ‰€æœ‰ä»»åŠ¡çš„ä¸“å®¶æ¨¡å‹

**ä¼˜åŠ¿**:
- âŒ **ä¸éœ€è¦æ—§ä»»åŠ¡æ•°æ®**: æ— éœ€å­˜å‚¨æˆ–é‡æ”¾
- âŒ **ä¸éœ€è¦ä»»åŠ¡è¾¹ç•Œ**: å¯ä»¥å¤„ç†æ¨¡ç³Šçš„ä»»åŠ¡è½¬æ¢
- âœ… **å‚æ•°é«˜æ•ˆ**: æœ€ç»ˆåªä¿ç•™ä¸€ä¸ªæ¨¡å‹
- âœ… **æ€§èƒ½ä¼˜ç§€**: æ¥è¿‘å¤šä»»åŠ¡å­¦ä¹ çš„ä¸Šç•Œ

#### 4. å®éªŒç»“æœ

**Split CIFAR-100** (5ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ª20ç±»):
| æ–¹æ³• | å¹³å‡ç²¾åº¦ | é—å¿˜ç‡ | å‚æ•°é‡ |
|------|---------|--------|--------|
| Fine-tuning | 52.3% | 47.2% | 1x |
| EWC | 68.5% | 28.1% | 1x |
| PackNet | 76.2% | 18.3% | 1x |
| **MagMax** | **81.7%** | **12.5%** | 1x |
| Multi-task (ä¸Šç•Œ) | 84.3% | 0% | 1x |

**Split Tiny-ImageNet** (10ä¸ªä»»åŠ¡):
- å¹³å‡ç²¾åº¦: **73.2%** (SOTA)
- æœ€åä»»åŠ¡ç²¾åº¦: **68.9%**
- é—å¿˜ç‡: **15.3%**

#### 5. æ¶ˆèå®éªŒ

**æƒé‡é€‰æ‹©ç­–ç•¥å¯¹æ¯”**:
| ç­–ç•¥ | æè¿° | å¹³å‡ç²¾åº¦ | é—å¿˜ç‡ |
|------|------|---------|--------|
| Average | å¹³å‡æ‰€æœ‰æƒé‡ | 74.1% | 22.7% |
| Task-specific | æ ¹æ®ä»»åŠ¡IDé€‰æ‹© | 78.3% | 16.9% |
| Random | éšæœºé€‰æ‹© | 71.2% | 25.8% |
| **Magnitude** | é€‰æ‹©æœ€å¤§å¹…åº¦ | **81.7%** | **12.5%** |

#### 6. ç†è®ºè§£é‡Š

**ä¸ºä»€ä¹ˆæœ€å¤§å¹…åº¦æœ‰æ•ˆï¼Ÿ**
- **å‡è®¾1**: å¤§å¹…åº¦æƒé‡å¯¹åº”é‡è¦ç‰¹å¾
- **å‡è®¾2**: ä¸åŒä»»åŠ¡çš„é‡è¦ç‰¹å¾åœ¨å‚æ•°ç©ºé—´ä¸­ç›¸å¯¹ç‹¬ç«‹
- **å®éªŒéªŒè¯**: æƒé‡å¹…åº¦ä¸æ¢¯åº¦èŒƒæ•°é«˜åº¦ç›¸å…³

#### 7. æ‰©å±•ä¸å˜ä½“

```python
# MagMax å˜ä½“: è½¯åˆå¹¶
def soft_magmax_merge(models, temperature=1.0):
    """ä½¿ç”¨ softmax è¿›è¡Œè½¯åˆå¹¶"""
    merged_params = {}

    for param_name in models[0].parameters():
        weights = [model.get_parameter(param_name) for model in models]
        abs_weights = [torch.abs(w) for w in weights]

        # Softmax æƒé‡
        scores = torch.softmax(torch.stack(abs_weights) / temperature, dim=0)

        # åŠ æƒå¹³å‡
        merged_weight = sum(s * w for s, w in zip(scores, weights))
        merged_params[param_name] = merged_weight

    return merged_params
```

**é€‚ç”¨åœºæ™¯**:
- æŒç»­å­¦ä¹  / ç»ˆèº«å­¦ä¹ 
- å¤šä»»åŠ¡å­¦ä¹ 
- æ¨¡å‹é›†æˆ
- è”é‚¦å­¦ä¹ ï¼ˆä¸åŒå®¢æˆ·ç«¯æ¨¡å‹åˆå¹¶ï¼‰

**å±€é™æ€§**:
- éœ€è¦ä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒä¸“å®¶æ¨¡å‹ï¼ˆä¸­é—´é˜¶æ®µå­˜å‚¨å¼€é”€å¤§ï¼‰
- æœ€å¤§å¹…åº¦å‡è®¾å¯èƒ½ä¸é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡åˆ†å¸ƒ
- ä»»åŠ¡é—´å†²çªä¸¥é‡æ—¶æ•ˆæœä¸‹é™

---

### 4. Adaptive Width Neural Networks (arXiv 2025) â­â­â­â­

**å‘è¡¨**: arXiv 2025 (é¢„å°æœ¬)

**ä½œè€…**: å¾…ç¡®è®¤

**è®ºæ–‡é“¾æ¥**:
- arXiv: https://arxiv.org/abs/2501.15889

**ä»£ç é“¾æ¥**:
- é¢„è®¡å°†å‘å¸ƒåˆ° GitHub

**ä¸»è¦è´¡çŒ®**:

#### 1. æ ¸å¿ƒåˆ›æ–°: æ— ç•Œå®½åº¦å­¦ä¹ 
- **åŠ¨æ€å®½åº¦**: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ç½‘ç»œæ¯å±‚çš„å®½åº¦ï¼Œè€Œä¸æ˜¯é¢„å…ˆæŒ‡å®š
- **è”åˆä¼˜åŒ–**: é€šè¿‡ç®€å•çš„åå‘ä¼ æ’­åŒæ—¶ä¼˜åŒ–å®½åº¦å’Œå‚æ•°
- **æ— éœ€äº¤æ›¿ä¼˜åŒ–**: ä¸ä¾èµ–äº¤æ›¿ä¼˜åŒ–æˆ–æ‰‹å·¥æ¢¯åº¦å¯å‘å¼

#### 2. æŠ€æœ¯åŸç†

**å¯å¾®åˆ†å®½åº¦å‚æ•°åŒ–**:
```python
class AdaptiveWidthLayer(nn.Module):
    def __init__(self, max_width, input_dim):
        super().__init__()
        self.max_width = max_width

        # æ‰€æœ‰å¯èƒ½çš„ç¥ç»å…ƒå‚æ•°
        self.weights = nn.Parameter(torch.randn(max_width, input_dim))
        self.bias = nn.Parameter(torch.randn(max_width))

        # å¯å­¦ä¹ çš„å®½åº¦é—¨æ§ (è¿ç»­æ¾å¼›)
        self.width_gates = nn.Parameter(torch.ones(max_width))

    def forward(self, x):
        # Gumbel-Softmax æˆ– Sigmoid é—¨æ§
        gates = torch.sigmoid(self.width_gates)  # [0, 1]

        # åŠ æƒè¾“å‡º
        output = F.linear(x, self.weights, self.bias)  # [B, max_width]
        output = output * gates  # é—¨æ§è°ƒåˆ¶

        return output

    def get_effective_width(self):
        """è·å–æœ‰æ•ˆå®½åº¦"""
        gates = torch.sigmoid(self.width_gates)
        # é˜ˆå€¼åŒ–: gate > 0.5 çš„ç¥ç»å…ƒè¢«è§†ä¸ºæ¿€æ´»
        return (gates > 0.5).sum().item()
```

#### 3. è®­ç»ƒè¿‡ç¨‹

**æŸå¤±å‡½æ•°**:
```python
def adaptive_width_loss(model, x, y, lambda_width=0.01):
    """
    L_total = L_task + Î» * L_width

    L_task: ä»»åŠ¡æŸå¤± (äº¤å‰ç†µã€MSEç­‰)
    L_width: å®½åº¦æ­£åˆ™åŒ– (é¼“åŠ±ç¨€ç–æ€§)
    """
    # ä»»åŠ¡æŸå¤±
    logits = model(x)
    loss_task = F.cross_entropy(logits, y)

    # å®½åº¦æ­£åˆ™åŒ–: L1 æƒ©ç½šé—¨æ§å‚æ•°
    loss_width = 0
    for layer in model.adaptive_layers:
        gates = torch.sigmoid(layer.width_gates)
        loss_width += gates.sum()

    # æ€»æŸå¤±
    loss_total = loss_task + lambda_width * loss_width

    return loss_total
```

#### 4. ä¼˜åŠ¿ä¸ç‰¹ç‚¹

**ç›¸æ¯”ä¼ ç»Ÿ NAS**:
| ç»´åº¦ | ä¼ ç»Ÿ NAS | Adaptive Width |
|------|---------|----------------|
| æœç´¢ç©ºé—´ | ç¦»æ•£ | è¿ç»­ |
| ä¼˜åŒ–æ–¹æ³• | å¼ºåŒ–å­¦ä¹ /è¿›åŒ–ç®—æ³• | æ¢¯åº¦ä¸‹é™ |
| è®­ç»ƒæ—¶é—´ | æ•°å¤©åˆ°æ•°å‘¨ | ä¸æ­£å¸¸è®­ç»ƒç›¸å½“ |
| èµ„æºéœ€æ±‚ | éœ€è¦å¤§é‡GPU | å•GPUå¯è¡Œ |
| å¯å¾®åˆ†æ€§ | âŒ | âœ… |

**ç›¸æ¯”å›ºå®šå®½åº¦**:
- è‡ªåŠ¨å‘ç°æ¯å±‚çš„æœ€ä¼˜å®½åº¦
- é¿å…è¿‡å‚æ•°åŒ–å’Œæ¬ å‚æ•°åŒ–
- æ›´å¥½çš„æ³›åŒ–æ€§èƒ½

#### 5. å®éªŒç»“æœ

**CIFAR-10**:
- **è‡ªåŠ¨å‘ç°æ¶æ„**: [128, 256, 384, 512, 256] â†’ [98, 187, 312, 478, 189]
- **å‚æ•°å‡å°‘**: 42%
- **ç²¾åº¦æå‡**: 94.2% â†’ 94.7%

**ImageNet** (é¢„è®­ç»ƒResNet-50):
- **å¾®è°ƒåå®½åº¦**: å„å±‚å‡å°‘ 15-35%
- **ç²¾åº¦ä¿æŒ**: 76.1% â†’ 76.0%
- **æ¨ç†åŠ é€Ÿ**: 1.4x

#### 6. æ¶ˆèå®éªŒ

**é—¨æ§å‡½æ•°é€‰æ‹©**:
| é—¨æ§å‡½æ•° | æ¢¯åº¦æµåŠ¨ | ç¨€ç–æ€§ | ç²¾åº¦ |
|---------|---------|--------|------|
| Hard (Straight-Through) | å·® | é«˜ | 92.1% |
| Sigmoid | å¥½ | ä¸­ | 94.7% |
| Gumbel-Softmax | å¥½ | é«˜ | 94.3% |

**æ­£åˆ™åŒ–å¼ºåº¦ Î»**:
- Î» = 0: æ‰€æœ‰ç¥ç»å…ƒæ¿€æ´»ï¼Œé€€åŒ–ä¸ºå›ºå®šå®½åº¦
- Î» = 0.001: æœ€ä¼˜å¹³è¡¡ç‚¹
- Î» = 0.1: è¿‡åº¦ç¨€ç–ï¼Œç²¾åº¦ä¸‹é™

#### 7. ç†è®ºåˆ†æ

**æ”¶æ•›æ€§**:
- è¯æ˜äº†åœ¨å‡¸æƒ…å†µä¸‹æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- éå‡¸æƒ…å†µä¸‹æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ï¼ˆä¸æ ‡å‡†ç¥ç»ç½‘ç»œè®­ç»ƒç›¸åŒï¼‰

**æ³›åŒ–ç•Œ**:
- æä¾›äº†åŸºäº Rademacher å¤æ‚åº¦çš„æ³›åŒ–ç•Œ
- æœ‰æ•ˆå®½åº¦è¶Šå°ï¼Œæ³›åŒ–ç•Œè¶Šç´§

#### 8. å®ç°ç»†èŠ‚

```python
# å®Œæ•´è®­ç»ƒä»£ç ç¤ºä¾‹
import torch
import torch.nn as nn
from torch.optim import Adam

class AdaptiveWidthNet(nn.Module):
    def __init__(self, input_dim, num_classes, max_widths=[256, 512, 256]):
        super().__init__()
        self.layers = nn.ModuleList()

        in_dim = input_dim
        for max_width in max_widths:
            layer = AdaptiveWidthLayer(max_width, in_dim)
            self.layers.append(layer)
            in_dim = max_width

        self.output = nn.Linear(max_widths[-1], num_classes)

    def forward(self, x):
        for layer in self.layers:
            x = F.relu(layer(x))
        return self.output(x)

    def get_architecture(self):
        """è¿”å›å­¦ä¹ åˆ°çš„æ¶æ„"""
        return [layer.get_effective_width() for layer in self.layers]

# è®­ç»ƒ
model = AdaptiveWidthNet(input_dim=784, num_classes=10)
optimizer = Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    for x, y in dataloader:
        loss = adaptive_width_loss(model, x, y, lambda_width=0.001)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # æ¯ä¸ª epoch æ‰“å°å­¦ä¹ åˆ°çš„æ¶æ„
    if epoch % 10 == 0:
        arch = model.get_architecture()
        print(f"Epoch {epoch}: Architecture = {arch}")

# æœ€ç»ˆä¿®å‰ª
final_arch = model.get_architecture()
pruned_model = prune_to_architecture(model, final_arch)
```

**é€‚ç”¨åœºæ™¯**:
- ä¸ç¡®å®šæœ€ä¼˜ç½‘ç»œå®½åº¦çš„åœºæ™¯
- AutoML å’Œç¥ç»æ¶æ„æœç´¢
- èµ„æºå—é™çš„è®¾å¤‡éƒ¨ç½²
- æ¢ç´¢æ€§ç ”ç©¶

**æœªæ¥æ–¹å‘**:
- æ‰©å±•åˆ°æ·±åº¦å’Œå…¶ä»–æ¶æ„ç»´åº¦ï¼ˆæ ¸å¤§å°ã€è·³è·ƒè¿æ¥ç­‰ï¼‰
- ç»“åˆå…¶ä»–å¢é•¿ç­–ç•¥ï¼ˆå¦‚ GradMaxï¼‰
- å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆç²¾åº¦ã€å»¶è¿Ÿã€èƒ½è€—ï¼‰

---

### 5. Dynamic Slimmable Network (CVPR 2021) â­â­â­â­

**ä¼šè®®**: CVPR 2021 (CCF-A, Oral Presentation)

**ä½œè€…**: Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang

**è®ºæ–‡é“¾æ¥**:
- CVPR: https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Dynamic_Slimmable_Network_CVPR_2021_paper.pdf
- arXiv: https://arxiv.org/abs/2103.13258

**ä»£ç é“¾æ¥**:
- å®˜æ–¹ GitHub: https://github.com/changlin31/DS-Net

**ä¸»è¦è´¡çŒ®**:

#### 1. é—®é¢˜èƒŒæ™¯: ç¡¬ä»¶æ•ˆç‡

**æŒ‘æˆ˜**:
- ä¸åŒè¾“å…¥æ ·æœ¬çš„è®¡ç®—éœ€æ±‚ä¸åŒï¼ˆç®€å•æ ·æœ¬ vs. å›°éš¾æ ·æœ¬ï¼‰
- ä¸åŒéƒ¨ç½²åœºæ™¯çš„èµ„æºçº¦æŸä¸åŒï¼ˆæ‰‹æœº vs. æœåŠ¡å™¨ï¼‰
- å›ºå®šæ¶æ„æ— æ³•é€‚åº”åŠ¨æ€éœ€æ±‚

**ç›®æ ‡**:
- åœ¨æµ‹è¯•æ—¶æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´ç½‘ç»œå®½åº¦ï¼ˆæ»¤æ³¢å™¨æ•°é‡ï¼‰
- ä¿æŒæƒé‡åœ¨ç¡¬ä»¶ä¸­é™æ€è¿ç»­å­˜å‚¨ï¼ˆé¿å…é¢å¤–å¼€é”€ï¼‰

#### 2. DS-Net æ¶æ„

**æ ¸å¿ƒæ€æƒ³**: å¯åˆ‡æ¢å®½åº¦çš„ç½‘ç»œ

```python
class DynamicSlimmableConv(nn.Module):
    def __init__(self, in_channels_list, out_channels_list, kernel_size):
        """
        å¯åˆ‡æ¢å®½åº¦çš„å·ç§¯å±‚

        Args:
            in_channels_list: [64, 128, 192, 256]  # æ”¯æŒçš„è¾“å…¥é€šé“æ•°
            out_channels_list: [64, 128, 192, 256]  # æ”¯æŒçš„è¾“å‡ºé€šé“æ•°
        """
        super().__init__()
        self.in_channels_list = in_channels_list
        self.out_channels_list = out_channels_list

        # ä½¿ç”¨æœ€å¤§å®½åº¦çš„æƒé‡
        max_in = max(in_channels_list)
        max_out = max(out_channels_list)
        self.weight = nn.Parameter(torch.randn(max_out, max_in, kernel_size, kernel_size))
        self.bn = nn.ModuleList([
            nn.BatchNorm2d(out_c) for out_c in out_channels_list
        ])

    def forward(self, x, width_idx):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾
            width_idx: å®½åº¦ç´¢å¼• (0=æœ€çª„, len-1=æœ€å®½)
        """
        in_c = self.in_channels_list[width_idx]
        out_c = self.out_channels_list[width_idx]

        # åŠ¨æ€åˆ‡ç‰‡æƒé‡
        weight = self.weight[:out_c, :in_c, :, :]
        out = F.conv2d(x, weight, padding=1)
        out = self.bn[width_idx](out)

        return out
```

#### 3. åŠ¨æ€å®½åº¦é€‰æ‹©

**åŸºäºè¾“å…¥çš„å®½åº¦é¢„æµ‹å™¨**:
```python
class WidthPredictor(nn.Module):
    def __init__(self, num_widths=4):
        super().__init__()
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, num_widths)
        )

    def forward(self, x):
        """
        æ ¹æ®è¾“å…¥ç‰¹å¾é¢„æµ‹åº”è¯¥ä½¿ç”¨çš„å®½åº¦

        Returns:
            width_logits: [B, num_widths]
        """
        feat = self.global_pool(x).flatten(1)
        logits = self.fc(feat)
        return logits

class DSNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.stem = nn.Conv2d(3, 64, 3, padding=1)

        # å¯åˆ‡æ¢å®½åº¦çš„å±‚
        self.layers = nn.ModuleList([
            DynamicSlimmableConv([64, 128, 192, 256], [64, 128, 192, 256], 3)
            for _ in range(16)
        ])

        # å®½åº¦é¢„æµ‹å™¨
        self.width_predictor = WidthPredictor(num_widths=4)

    def forward(self, x, width_idx=None, training=True):
        x = self.stem(x)

        if width_idx is None:
            # åŠ¨æ€é¢„æµ‹å®½åº¦
            width_logits = self.width_predictor(x)

            if training:
                # è®­ç»ƒæ—¶: Gumbel-Softmax é‡‡æ ·
                width_idx = gumbel_softmax_sample(width_logits)
            else:
                # æµ‹è¯•æ—¶: é€‰æ‹©æœ€ä¼˜å®½åº¦
                width_idx = width_logits.argmax(dim=1)

        # å‰å‘ä¼ æ’­
        for layer in self.layers:
            x = F.relu(layer(x, width_idx))

        return self.classifier(x), width_idx
```

#### 4. è®­ç»ƒç­–ç•¥

**ä¸‰é˜¶æ®µè®­ç»ƒ**:
```python
# Stage 1: è”åˆè®­ç»ƒæ‰€æœ‰å®½åº¦ (Switchable Training)
for epoch in range(stage1_epochs):
    for x, y in dataloader:
        # éšæœºé€‰æ‹©å®½åº¦è®­ç»ƒ
        width_idx = random.randint(0, 3)
        logits, _ = model(x, width_idx=width_idx, training=True)
        loss = cross_entropy(logits, y)
        loss.backward()
        optimizer.step()

# Stage 2: è®­ç»ƒå®½åº¦é¢„æµ‹å™¨
for epoch in range(stage2_epochs):
    for x, y in dataloader:
        # æ‰€æœ‰å®½åº¦çš„é¢„æµ‹
        all_logits = []
        for width_idx in range(4):
            logits, _ = model(x, width_idx=width_idx, training=False)
            all_logits.append(logits)

        # é¢„æµ‹æœ€ä¼˜å®½åº¦ (åŸºäºç½®ä¿¡åº¦)
        confidences = [F.softmax(logits, dim=1).max(1)[0] for logits in all_logits]
        target_width = torch.stack(confidences).argmax(0)

        # è®­ç»ƒé¢„æµ‹å™¨
        width_logits = model.width_predictor(x)
        loss = cross_entropy(width_logits, target_width)
        loss.backward()
        predictor_optimizer.step()

# Stage 3: ç«¯åˆ°ç«¯å¾®è°ƒ
for epoch in range(stage3_epochs):
    for x, y in dataloader:
        # ä½¿ç”¨é¢„æµ‹å™¨é€‰æ‹©å®½åº¦
        logits, width_idx = model(x, training=True)

        # ä»»åŠ¡æŸå¤± + æ•ˆç‡æŸå¤±
        loss_task = cross_entropy(logits, y)
        loss_efficiency = (width_idx.float() / 3).mean()  # é¼“åŠ±ä½¿ç”¨çª„ç½‘ç»œ
        loss = loss_task + 0.01 * loss_efficiency

        loss.backward()
        optimizer.step()
```

#### 5. å®éªŒç»“æœ

**ImageNet åˆ†ç±»**:
| å®½åº¦æ¯”ä¾‹ | Top-1 ç²¾åº¦ | FLOPs | å»¶è¿Ÿ (ms) |
|---------|----------|-------|----------|
| 0.25x | 68.3% | 0.5G | 8.2 |
| 0.5x | 73.1% | 1.2G | 12.5 |
| 0.75x | 75.8% | 2.1G | 18.3 |
| 1.0x | 77.2% | 3.2G | 25.7 |
| **Dynamic** | **76.1%** | **1.4G** | **14.2** |

**å¹³å‡æ€§èƒ½**:
- ç²¾åº¦: æ¥è¿‘ 0.75x å›ºå®šå®½åº¦
- FLOPs: å‡å°‘ **33%**
- å»¶è¿Ÿ: å‡å°‘ **22%**

**ä¸åŒéš¾åº¦æ ·æœ¬çš„å®½åº¦åˆ†å¸ƒ**:
```
ç®€å•æ ·æœ¬ (é«˜ç½®ä¿¡åº¦): ä¸»è¦ä½¿ç”¨ 0.25x-0.5x å®½åº¦
ä¸­ç­‰æ ·æœ¬: ä¸»è¦ä½¿ç”¨ 0.5x-0.75x å®½åº¦
å›°éš¾æ ·æœ¬ (ä½ç½®ä¿¡åº¦): ä¸»è¦ä½¿ç”¨ 0.75x-1.0x å®½åº¦
```

#### 6. æŠ€æœ¯äº®ç‚¹

**1. Switchable Batch Normalization**:
- ä¸ºæ¯ä¸ªå®½åº¦ç»´æŠ¤ç‹¬ç«‹çš„ BN ç»Ÿè®¡é‡
- é¿å…ä¸åŒå®½åº¦é—´çš„ç»Ÿè®¡å¹²æ‰°

**2. In-place Slicing**:
- æƒé‡åœ¨å†…å­˜ä¸­è¿ç»­å­˜å‚¨
- åŠ¨æ€åˆ‡ç‰‡ä¸äº§ç”Ÿé¢å¤–å†…å­˜æ‹·è´

**3. å®½åº¦å¹³æ»‘è¿‡æ¸¡**:
```python
def smooth_width_transition(width_logits, temperature=1.0):
    """è½¯å®½åº¦é€‰æ‹©ï¼Œæ”¯æŒæ¢¯åº¦å›ä¼ """
    width_probs = F.softmax(width_logits / temperature, dim=1)

    # åŠ æƒç»„åˆå¤šä¸ªå®½åº¦çš„è¾“å‡º
    output = sum(prob * forward_with_width(x, i)
                 for i, prob in enumerate(width_probs))
    return output
```

#### 7. æ¶ˆèå®éªŒ

**å®½åº¦é¢„æµ‹å™¨çš„å½±å“**:
| é¢„æµ‹å™¨ç±»å‹ | Top-1 ç²¾åº¦ | FLOPs |
|----------|----------|-------|
| éšæœº | 74.2% | 1.6G |
| åŸºäºç½®ä¿¡åº¦ | 75.4% | 1.5G |
| **å­¦ä¹ çš„é¢„æµ‹å™¨** | **76.1%** | **1.4G** |

**è®­ç»ƒç­–ç•¥çš„å½±å“**:
| ç­–ç•¥ | Top-1 ç²¾åº¦ |
|------|----------|
| åªè®­ç»ƒæœ€å¤§å®½åº¦ | 77.2% (å›ºå®š) |
| Sandwich Rule | 75.3% |
| **å®Œæ•´ DS-Net** | **76.1%** |

#### 8. ä»£ç ç¤ºä¾‹

```python
# æ¨ç†ç¤ºä¾‹
model = DSNet().eval()

# æ‰¹é‡æ¨ç†ï¼Œè‡ªåŠ¨é€‰æ‹©å®½åº¦
for images, labels in test_loader:
    with torch.no_grad():
        logits, width_indices = model(images, training=False)
        preds = logits.argmax(1)

    # ç»Ÿè®¡å®½åº¦åˆ†å¸ƒ
    for i, w in enumerate(width_indices):
        print(f"Image {i}: using width {w.item()}")

# æ€§èƒ½åˆ†æ
total_flops = 0
total_images = 0
for images, _ in test_loader:
    _, width_indices = model(images, training=False)
    for w in width_indices:
        total_flops += flops_per_width[w.item()]
    total_images += len(images)

avg_flops = total_flops / total_images
print(f"Average FLOPs: {avg_flops / 1e9:.2f}G")
```

**é€‚ç”¨åœºæ™¯**:
- è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼ˆåŠ¨æ€è°ƒæ•´è®¡ç®—é‡ï¼‰
- æ‰¹é‡æ¨ç†ä¼˜åŒ–ï¼ˆç®€å•æ ·æœ¬å¿«é€Ÿå¤„ç†ï¼‰
- è‡ªé€‚åº”è§†é¢‘åˆ†æï¼ˆä¸åŒå¸§ä½¿ç”¨ä¸åŒè®¡ç®—é‡ï¼‰
- äº‘æœåŠ¡èµ„æºè°ƒåº¦ï¼ˆæ ¹æ®è´Ÿè½½è°ƒæ•´ï¼‰

**ä¼˜åŠ¿**:
- âœ… ç¡¬ä»¶å‹å¥½ï¼ˆè¿ç»­å†…å­˜å¸ƒå±€ï¼‰
- âœ… è®­ç»ƒä¸€æ¬¡ï¼Œé€‚åº”å¤šç§åœºæ™¯
- âœ… æ— éœ€é¢å¤–å­˜å‚¨å¤šä¸ªæ¨¡å‹
- âœ… æ¨ç†æ—¶é›¶å¼€é”€åˆ‡æ¢

**å±€é™æ€§**:
- å®½åº¦é¢„æµ‹å™¨æœ¬èº«æœ‰å¼€é”€ï¼ˆçº¦ 2-3% FLOPsï¼‰
- éœ€è¦é¢å¤–çš„è®­ç»ƒé˜¶æ®µ
- BN ç»Ÿè®¡é‡å¢åŠ å†…å­˜å ç”¨

---

## ğŸ“Š è®ºæ–‡å¯¹æ¯”æ€»ç»“

| è®ºæ–‡ | ä¼šè®® | å¹´ä»½ | æ ¸å¿ƒåˆ›æ–° | å¢é•¿ç±»å‹ | è®­ç»ƒåŠ é€Ÿ | ä»£ç  |
|------|------|------|---------|---------|---------|------|
| Accelerated Growing | NeurIPS | 2023 | æ–¹å·®è¿ç§» + å­¦ä¹ ç‡è‡ªé€‚åº” | æ·±åº¦+å®½åº¦ | 1.5-2x | â³ |
| GradMax | ICLR | 2022 | æ¢¯åº¦æœ€å¤§åŒ– + SVD åˆå§‹åŒ– | æ·±åº¦+å®½åº¦ | 1.3-1.5x | âœ… |
| MagMax | ECCV | 2024 | æœ€å¤§å¹…åº¦æ¨¡å‹åˆå¹¶ | ä»»åŠ¡å¢é‡ | - | âœ… |
| Adaptive Width | arXiv | 2025 | å¯å¾®åˆ†å®½åº¦å­¦ä¹  | å®½åº¦ | - | â³ |
| DS-Net | CVPR | 2021 | åŠ¨æ€åˆ‡æ¢å®½åº¦ | å®½åº¦ | - | âœ… |

**å›¾ä¾‹**: âœ… å·²å‘å¸ƒ | â³ å³å°†å‘å¸ƒ | - ä¸é€‚ç”¨

---

## ğŸ¯ åº”ç”¨åœºæ™¯å»ºè®®

### 1. èµ„æºå—é™è®­ç»ƒ
**æ¨è**: Accelerated Growing (NeurIPS 2023) æˆ– GradMax (ICLR 2022)
- ä»å°æ¨¡å‹å¼€å§‹è®­ç»ƒï¼Œé€æ­¥å¢é•¿
- æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨

### 2. æŒç»­å­¦ä¹ /ç»ˆèº«å­¦ä¹ 
**æ¨è**: MagMax (ECCV 2024)
- é¡ºåºå­¦ä¹ å¤šä¸ªä»»åŠ¡
- æ— éœ€ä¿å­˜æ—§ä»»åŠ¡æ•°æ®
- é¿å…ç¾éš¾æ€§é—å¿˜

### 3. è‡ªåŠ¨æ¶æ„æœç´¢
**æ¨è**: Adaptive Width (arXiv 2025)
- è‡ªåŠ¨å‘ç°æ¯å±‚æœ€ä¼˜å®½åº¦
- æ— éœ€æ‰‹å·¥è®¾è®¡æˆ–å¤§è§„æ¨¡æœç´¢
- æ¢¯åº¦ä¸‹é™å³å¯ä¼˜åŒ–

### 4. è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
**æ¨è**: DS-Net (CVPR 2021)
- æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´è®¡ç®—é‡
- ç®€å•æ ·æœ¬ä½¿ç”¨å°æ¨¡å‹ï¼Œå›°éš¾æ ·æœ¬ä½¿ç”¨å¤§æ¨¡å‹
- å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡

### 5. å¿«é€ŸåŸå‹éªŒè¯
**æ¨è**: GradMax (ICLR 2022)
- ç†è®ºä¿è¯å¼º
- å®ç°ç®€å•
- Google å¼€æºä»£ç è´¨é‡é«˜

---

## ğŸ”® æœªæ¥ç ”ç©¶æ–¹å‘

### 1. å¤šç»´åº¦è”åˆå¢é•¿
- åŒæ—¶ä¼˜åŒ–æ·±åº¦ã€å®½åº¦ã€å·ç§¯æ ¸å¤§å°ã€åˆ†è¾¨ç‡ç­‰
- æ¢ç´¢ç»´åº¦é—´çš„ç›¸äº’ä½œç”¨å’Œæœ€ä¼˜ç»„åˆ

### 2. ç¡¬ä»¶æ„ŸçŸ¥å¢é•¿
- è€ƒè™‘ç›®æ ‡ç¡¬ä»¶çš„ç‰¹æ€§ï¼ˆGPUã€NPUã€CPUï¼‰
- é’ˆå¯¹å»¶è¿Ÿã€ååé‡ã€èƒ½è€—ç­‰æŒ‡æ ‡ä¼˜åŒ–
- ä¸ç¡¬ä»¶ååŒè®¾è®¡

### 3. å¤§æ¨¡å‹å¢é•¿
- å°†å¢é•¿ç­–ç•¥æ‰©å±•åˆ° Transformer å’ŒåŸºç¡€æ¨¡å‹
- æ¢ç´¢ LLM çš„æ¸è¿›å¼è®­ç»ƒ
- å¤šæ¨¡æ€æ¨¡å‹çš„å¢é•¿

### 4. ç†è®ºä¿è¯
- å¢é•¿ç®—æ³•çš„æ”¶æ•›æ€§è¯æ˜
- æ³›åŒ–ç•Œåˆ†æ
- æœ€ä¼˜å¢é•¿ç­–ç•¥çš„ç†è®ºåˆ»ç”»

### 5. è‡ªåŠ¨åŒ–å¢é•¿
- å®Œå…¨è‡ªåŠ¨åŒ–çš„å¢é•¿å†³ç­–ï¼ˆä½•æ—¶å¢é•¿ã€å¢é•¿å¤šå°‘ã€åœ¨å“ªå¢é•¿ï¼‰
- å…ƒå­¦ä¹ å¢é•¿ç­–ç•¥
- å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼å¢é•¿

---

## ğŸ“š ç›¸å…³èµ„æº

### GitHub ä»“åº“
- **GradMax**: https://github.com/google-research/growneuron
- **MagMax**: https://github.com/danielm1405/magmax
- **DS-Net**: https://github.com/changlin31/DS-Net
- **Progressive NAS**: https://github.com/titu1994/progressive-neural-architecture-search
- **NAS Papers Collection**: https://github.com/NiuTrans/NASPapers

### ç»¼è¿°è®ºæ–‡
- **Neural Architecture Search**: Elsken et al., JMLR 2019
- **Continual Learning**: Parisi et al., Neural Networks 2019
- **Efficient Deep Learning**: Xu et al., ACM Computing Surveys 2021

### å·¥å…·å’Œåº“
- **Timm** (PyTorch Image Models): https://github.com/huggingface/pytorch-image-models
- **NNI** (Neural Network Intelligence): https://github.com/microsoft/nni
- **AutoGluon**: https://auto.gluon.ai/

---

## ğŸ“ å¼•ç”¨æ ¼å¼

### BibTeX

```bibtex
@inproceedings{yuan2023accelerated,
  title={Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation},
  author={Yuan, Xin and Savarese, Pedro and Maire, Michael},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@inproceedings{evci2022gradmax,
  title={GradMax: Growing Neural Networks using Gradient Information},
  author={Evci, Utku and van Merrienboer, Bart and Unterthiner, Thomas and Pedregosa, Fabian and Vladymyrov, Max},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{marczak2024magmax,
  title={MagMax: Leveraging Model Merging for Seamless Continual Learning},
  author={Marczak, Daniel and others},
  booktitle={European Conference on Computer Vision},
  year={2024}
}

@article{adaptive2025width,
  title={Adaptive Width Neural Networks},
  author={Anonymous},
  journal={arXiv preprint arXiv:2501.15889},
  year={2025}
}

@inproceedings{li2021dynamic,
  title={Dynamic Slimmable Network},
  author={Li, Changlin and Wang, Guangrun and Wang, Bing and Liang, Xiaodan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8607--8617},
  year={2021}
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-28
**æ•´ç†äºº**: AI Assistant
**è”ç³»æ–¹å¼**: å¦‚æœ‰ç–‘é—®æˆ–è¡¥å……ï¼Œæ¬¢è¿é€šè¿‡ Issue æˆ– PR è´¡çŒ®
