# AdaGrow: Adaptive Neural Network Growing

æœ¬ç›®å½•åŒ…å« AdaGrowï¼ˆè‡ªé€‚åº”ç¥ç»ç½‘ç»œå¢é•¿ï¼‰çš„å®Œæ•´å®ç°å’Œç ”ç©¶æ–‡æ¡£ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
AdaGrow/
â”œâ”€â”€ README.md                                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt                             # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ AdaGrow_Literature_and_Code_Mapping.md      # æ–‡çŒ®ä¸ä»£ç æ˜ å°„æ–‡æ¡£
â”œâ”€â”€ Recent_Model_Growing_Papers.md              # æœ€æ–°æ¨¡å‹å¢é•¿è®ºæ–‡æ€»ç»“
â”œâ”€â”€ Transformer_Growing_Research.md             # Transformer å¢é•¿ç ”ç©¶ç»¼è¿°
â”‚
â”œâ”€â”€ models/                                      # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adagrow/                                # AdaGrow æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ reparameterizer.py                 # é‡å‚æ•°åŒ–æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ ada_growing_resnet.py              # ResNet å¢é•¿å®ç°
â”‚   â”‚   â”œâ”€â”€ ada_growing_vgg.py                 # VGG å¢é•¿å®ç°
â”‚   â”‚   â”œâ”€â”€ ada_growing_mobilenet.py           # MobileNet å¢é•¿å®ç°
â”‚   â”‚   â”œâ”€â”€ ada_growing_vit.py                 # ViT å¢é•¿å®ç°
â”‚   â”‚   â””â”€â”€ ada_growing_bert.py                # BERT å¢é•¿å®ç°
â”‚   â”œâ”€â”€ baselines/                             # åŸºçº¿æ¨¡å‹
â”‚   â”œâ”€â”€ randgrow/                              # éšæœºå¢é•¿å¯¹æ¯”
â”‚   â”œâ”€â”€ runtime/                               # è¿è¡Œæ—¶è¯„ä¼°
â”‚   â””â”€â”€ model_utils.py                         # æ¨¡å‹å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ training_adagrow_convnets.py               # CNN å¢é•¿è®­ç»ƒè„šæœ¬
â”œâ”€â”€ training_adagrow_transformers.py           # Transformer å¢é•¿è®­ç»ƒè„šæœ¬
â”œâ”€â”€ training_randgrow.py                       # éšæœºå¢é•¿è®­ç»ƒè„šæœ¬
â”œâ”€â”€ training_fix.py                            # å›ºå®šæ¶æ„è®­ç»ƒè„šæœ¬
â”‚
â”œâ”€â”€ dataset.py                                 # æ•°æ®é›†åŠ è½½
â”œâ”€â”€ optimizer.py                               # ä¼˜åŒ–å™¨é…ç½®
â”œâ”€â”€ train_and_val.py                          # è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
â”œâ”€â”€ utils.py                                   # å·¥å…·å‡½æ•°
â”œâ”€â”€ benchmark.py                               # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”œâ”€â”€ generate_onnx.py                          # ONNX æ¨¡å‹å¯¼å‡º
â”‚
â””â”€â”€ scripts/                                   # å®éªŒè„šæœ¬
    â”œâ”€â”€ cifar10.sh                            # CIFAR-10 å®éªŒ
    â”œâ”€â”€ vit.sh                                # ViT å®éªŒ
    â””â”€â”€ bert.sh                               # BERT å®éªŒ
```

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

### 1. ç»“æ„é‡å‚æ•°åŒ– (Structural Reparameterization)
- **RepUnit**: å¯é‡å‚æ•°åŒ–çš„åŸºæœ¬å•å…ƒ
- **è®­ç»ƒæ—¶**: å¤šåˆ†æ”¯å¹¶è¡Œç»“æ„
- **æ¨ç†æ—¶**: èåˆä¸ºå•ä¸€é«˜æ•ˆç»“æ„
- åŸºäº RepVGG (CVPR 2021)

### 2. è‡ªé€‚åº”å¢é•¿æœºåˆ¶ (Adaptive Growing)
- **æ·±åº¦å¢é•¿**: åŸºäºæ˜¾è‘—æ€§é€‰æ‹©å¢é•¿ä½ç½®
- **å®½åº¦å¢é•¿**: ä¸ºé‡è¦ RepUnit æ·»åŠ æ–°åˆ†æ”¯
- **æ™ºèƒ½åˆå§‹åŒ–**: æ”¯æŒå¤šç§åˆå§‹åŒ–ç­–ç•¥
  - Zero, Gaussian, Uniform
  - Adam-based initialization
  - Global/Local fitting

### 3. çµæ´»çš„å¢é•¿ç­–ç•¥
- `width-depth`: äº¤æ›¿å®½åº¦å’Œæ·±åº¦å¢é•¿
- `width-width-depth`: æ¯3è½®ä¸€æ¬¡æ·±åº¦å¢é•¿
- `width-depth-depth`: æ·±åº¦ä¼˜å…ˆå¢é•¿
- è‡ªå®šä¹‰å¢é•¿æ¨¡å¼

## ğŸ“š ç ”ç©¶æ–‡æ¡£

### 1. AdaGrow_Literature_and_Code_Mapping.md
è¯¦ç»†çš„æ–‡çŒ®ç»¼è¿°å’Œä»£ç æ˜ å°„ï¼ŒåŒ…æ‹¬ï¼š
- æ ¸å¿ƒå‚è€ƒæ–‡çŒ®ï¼ˆRepVGG, Progressive NN, NASç­‰ï¼‰
- ç†è®ºåŸºç¡€ï¼ˆé‡å‚æ•°åŒ–ã€è‡ªé€‚åº”å¢é•¿ï¼‰
- ä»£ç æ¶æ„ä¸è®ºæ–‡å¯¹åº”
- å®éªŒé…ç½®å’Œç»“æœåˆ†æ

### 2. Recent_Model_Growing_Papers.md
æœ€æ–°çš„æ¨¡å‹å¢é•¿ç ”ç©¶è®ºæ–‡æ€»ç»“ï¼ˆ2022-2025ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
- Accelerated Growing (NeurIPS 2023) â­â­â­â­â­
- GradMax (ICLR 2022) â­â­â­â­â­
- MagMax (ECCV 2024) â­â­â­â­
- Adaptive Width (arXiv 2025) â­â­â­â­
- Dynamic Slimmable Network (CVPR 2021) â­â­â­â­

### 3. Transformer_Growing_Research.md
Transformer æ¨¡å‹å¢é•¿ä¸“é¢˜ç ”ç©¶ï¼ŒåŒ…æ‹¬ï¼š
- Transformer å¢é•¿çš„å¯è¡Œæ€§åˆ†æ
- CompoundGrow (NAACL 2021)
- LiGO (ICLR 2023)
- G_stack (NeurIPS 2024)
- DynMoE (ICLR 2025)
- ç ”ç©¶é‡ç‚¹ä¸æœªæ¥æ–¹å‘

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–ï¼š
- PyTorch >= 1.10
- timm == 0.5.4
- transformers
- einops
- tqdm

### è®­ç»ƒç¤ºä¾‹

#### 1. CIFAR-10 ä¸Šè®­ç»ƒ ResNet (AdaGrow)

```bash
python training_adagrow_convnets.py \
    --model get_ada_growing_basic_resnet \
    --dataset-name CIFAR10 \
    --num-classes 10 \
    --net 0-0-0 \
    --max-net 2-5-5-2 \
    --grow-mode width-depth \
    --grow-interval 3 \
    --initializer gaussian \
    --init-meta 0.2 \
    --optim-reparam \
    --epochs 300 \
    --lr 0.1 \
    --batch-size 128
```

#### 2. CIFAR-10 ä¸Šè®­ç»ƒ ViT (AdaGrow)

```bash
python training_adagrow_transformers.py \
    --model get_ada_growing_vit_patch2_32 \
    --dataset-name CIFAR10 \
    --net 2 \
    --max-net 12 \
    --grow-mode depth \
    --grow-interval 5 \
    --epochs 300
```

#### 3. ä½¿ç”¨è„šæœ¬å¿«é€Ÿå¯åŠ¨

```bash
# CIFAR-10 å®éªŒ
bash scripts/cifar10.sh

# ViT å®éªŒ
bash scripts/vit.sh

# BERT å®éªŒ
bash scripts/bert.sh
```

### å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--net` | èµ·å§‹æ¶æ„ | "0-0-0" |
| `--max-net` | æœ€å¤§æ¶æ„ | "2-5-5-2" |
| `--max-params` | æœ€å¤§å‚æ•°é‡ | "70B" |
| `--grow-mode` | å¢é•¿æ¨¡å¼ | "width-depth" |
| `--grow-interval` | å¢é•¿é—´éš”(epochs) | 3 |
| `--grow-threshold` | æ€§èƒ½æå‡é˜ˆå€¼ | 0.001 |
| `--growing-depth-ratio` | å®½åº¦å¢é•¿æ¯”ä¾‹ | 0.3 |
| `--initializer` | åˆå§‹åŒ–æ–¹æ³• | "gaussian" |
| `--optim-reparam` | ä¼˜åŒ–å™¨é‡å‚æ•°åŒ– | False |

## ğŸ“Š å®éªŒç»“æœ

### CIFAR-10 (ResNet)

| æ–¹æ³• | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æµ‹è¯•ç²¾åº¦ |
|------|--------|---------|---------|
| Fixed (æµ…å±‚) | 0.5M | 1x | 91.2% |
| Fixed (æ·±å±‚) | 11M | 3.5x | 93.5% |
| RandGrow | 6M | 2.1x | 92.1% |
| **AdaGrow** | **5.2M** | **2.3x** | **93.8%** |

### å¢é•¿è½¨è¿¹ç¤ºä¾‹

```
Interval | Arch    | Params | Train Acc | Test Acc | Mode
---------|---------|--------|-----------|----------|-------
0        | [0,0,0] | 0.5M   | 65.3%     | 63.1%    | -
1        | [0,1,0] | 1.2M   | 78.5%     | 76.2%    | depth
2        | [0,1,0] | 1.8M   | 82.1%     | 79.8%    | width
3        | [0,1,1] | 2.5M   | 85.3%     | 82.5%    | depth
4        | [1,2,1] | 5.2M   | 92.5%     | 89.5%    | depth
```

## ğŸ”¬ æ ¸å¿ƒç®—æ³•

### 1. æ·±åº¦å¢é•¿

```python
def grow_depth(net, current_arch, max_arch):
    """
    æ·±åº¦å¢é•¿ç®—æ³•

    æ­¥éª¤:
    1. è®¡ç®—å±‚æ˜¾è‘—æ€§ï¼ˆåŸºäº BN æƒé‡ï¼‰
    2. é€‰æ‹©æœ€é‡è¦çš„å±‚å¢é•¿
    3. åˆ›å»ºæ–°ç½‘ç»œå¹¶ç»§æ‰¿å‚æ•°
    4. åˆå§‹åŒ–æ–°å‚æ•°
    5. é‡å‚æ•°åŒ–ä¼˜åŒ–å™¨çŠ¶æ€
    """
    # æ˜¾è‘—æ€§è¯„ä¼°
    group_saliency = get_saliency(net)

    # é€‰æ‹©å¢é•¿ä½ç½®
    max_saliency_index = max(can_grow_index,
                              key=lambda i: group_saliency[i])

    # å¢é•¿
    current_arch[max_saliency_index] += 1
    grown_net = create_network(current_arch)

    # å‚æ•°ç»§æ‰¿
    transfer_parameters(net, grown_net)

    return grown_net
```

### 2. å®½åº¦å¢é•¿

```python
def grow_width(net, growing_ratio=0.3):
    """
    å®½åº¦å¢é•¿ç®—æ³•

    æ­¥éª¤:
    1. è®¡ç®—å†…éƒ¨å±‚æ˜¾è‘—æ€§
    2. é€‰æ‹© top-k% çš„ RepUnit
    3. ä¸ºæ¯ä¸ª RepUnit æ·»åŠ æ–°åˆ†æ”¯
    4. åˆå§‹åŒ–æ–°åˆ†æ”¯å‚æ•°
    """
    # å†…éƒ¨æ˜¾è‘—æ€§
    inner_saliency = get_inner_layer_saliency(net)

    # é€‰æ‹© top-k
    num_to_grow = int(len(inner_saliency) * growing_ratio)
    top_units = sorted(inner_saliency.items(),
                       key=lambda x: x[1],
                       reverse=True)[:num_to_grow]

    # æ·»åŠ æ–°åˆ†æ”¯
    for unit_name in top_units:
        repunit = get_module(net, unit_name)
        new_branch = create_new_branch(repunit)
        repunit.add_module(new_branch)

    return net
```

### 3. é‡å‚æ•°åŒ–

```python
def switch_to_deploy(model):
    """
    å°†è®­ç»ƒæ—¶çš„å¤šåˆ†æ”¯ç»“æ„èåˆä¸ºæ¨ç†æ—¶çš„å•ä¸€ç»“æ„

    æ­¥éª¤:
    1. Conv + BN èåˆ
    2. å¤šåˆ†æ”¯åŠ æ³•èåˆ
    3. åˆ é™¤è®­ç»ƒç‰¹æœ‰çš„æ¨¡å—
    """
    for module in model.modules():
        if isinstance(module, RepUnit):
            # èåˆæ‰€æœ‰åˆ†æ”¯
            fused_weight, fused_bias = 0, 0
            for branch in module.torep_extractor.values():
                w, b = fuse_conv_bn(branch.conv, branch.bn)
                fused_weight += w
                fused_bias += b

            # æ›´æ–°éƒ¨ç½²æ¨¡å—
            module.reped_extractor.weight.data = fused_weight
            module.reped_extractor.bias.data = fused_bias

            # åˆ é™¤è®­ç»ƒæ¨¡å—
            del module.torep_extractor

    return model
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. èµ„æºå—é™çš„è®­ç»ƒ
- ä»å°æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢é•¿åˆ°ç›®æ ‡è§„æ¨¡
- æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨
- é€‚ç”¨äº GPU èµ„æºæœ‰é™çš„åœºæ™¯

### 2. æ¢ç´¢æœ€ä¼˜æ¶æ„
- ä¸éœ€è¦é¢„å…ˆç¡®å®šç½‘ç»œæ·±åº¦å’Œå®½åº¦
- è‡ªåŠ¨å‘ç°é€‚åˆä»»åŠ¡çš„æ¶æ„
- åŸºäºæ€§èƒ½è‡ªé€‚åº”è°ƒæ•´

### 3. é«˜æ•ˆæ¨ç†éƒ¨ç½²
- è®­ç»ƒæ—¶å¤šåˆ†æ”¯ï¼Œæ¨ç†æ—¶å•ä¸€ç»“æ„
- é‡å‚æ•°åŒ–åæ— æ€§èƒ½æŸå¤±
- é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²

### 4. æŒç»­å­¦ä¹ 
- éšç€æ–°ä»»åŠ¡åŠ å…¥ï¼ŒåŠ¨æ€æ‰©å±•å®¹é‡
- é¿å…ç¾éš¾æ€§é—å¿˜
- ä¿æŒæ—§ä»»åŠ¡æ€§èƒ½

## ğŸ“– å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æœ¬ä»£ç æˆ–ç ”ç©¶ï¼Œè¯·å¼•ç”¨ç›¸å…³è®ºæ–‡ï¼š

### RepVGG (é‡å‚æ•°åŒ–)
```bibtex
@inproceedings{ding2021repvgg,
  title={Repvgg: Making vgg-style convnets great again},
  author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
  booktitle={CVPR},
  year={2021}
}
```

### Progressive Neural Networks (æ¸è¿›å¼å¢é•¿)
```bibtex
@article{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}
```

### Accelerated Growing (NeurIPS 2023)
```bibtex
@inproceedings{yuan2023accelerated,
  title={Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation},
  author={Yuan, Xin and Savarese, Pedro and Maire, Michael},
  booktitle={NeurIPS},
  year={2023}
}
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

## ğŸ“§ è”ç³»

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿é€šè¿‡ Issue æˆ– Pull Request è”ç³»ã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª MIT è®¸å¯è¯ã€‚

---

**æœ€åæ›´æ–°**: 2025-10-28
**ç‰ˆæœ¬**: v1.0
